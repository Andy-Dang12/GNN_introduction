{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch, dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import DGLDataset\n",
    "import dgl.function as fn\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from colorama import Fore\n",
    "from random import shuffle\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from focal_loss import FocalLoss, focal_loss\n",
    "from dkkd_create_graph import classes, class_2_idx, class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. graphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_feat:int, out_feat:int):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : Graph\n",
    "            The input graph.\n",
    "        h : Tensor\n",
    "            The input node feature.\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # update_all is a message passing API.\n",
    "            g.update_all(message_func = fn.copy_u('h', 'm'), \n",
    "                         reduce_func = fn.mean('m', 'h_N'))\n",
    "                        #  reduce_func = fn.max('m', 'h_N'))\n",
    "                        #  reduce_func = fn.sum('m', 'h_N'))\n",
    "                        #  reduce_func = fn.sum('m', 'h_N'))\n",
    "            h_N = g.ndata['h_N']\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats_1, h_feats_2, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats_1)\n",
    "        self.conv2 = SAGEConv(h_feats_1, h_feats_2)\n",
    "        self.conv3 = SAGEConv(h_feats_2, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DkkdGraphDataset(DGLDataset):\n",
    "    def __init__(self, root:str='/home/agent/Documents/graph/GNN/dataset/DKKD_graph'):\n",
    "        super().__init__(name='dataset/DKKD_graph')\n",
    "        self.root = root\n",
    "        self.edges = glob(osp.join(root, '*.edges.csv'))\n",
    "        # self.nodes_feat = glob(osp.join(root, '*.nfeat.npy'))\n",
    "        # self.nodes_label = glob(osp.join(root, '*.idx.csv'))\n",
    "        self.shuffle()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_n_nodes(nodes_label:pd.DataFrame) -> int:\n",
    "        r\"\"\"\n",
    "        tính và kiểm tra số thứ tự của node\n",
    "        \"\"\"\n",
    "        n_nodes = nodes_label['Id'].to_list()\n",
    "        for i, idx in enumerate(n_nodes):\n",
    "            assert i == idx, 'i != idx'\n",
    "        return len(n_nodes)\n",
    "    \n",
    "    def __len__(self): return len(self.edges)\n",
    "    \n",
    "    def __getitem__(self, i) -> dgl.DGLGraph:\n",
    "        edgep = self.edges[i]\n",
    "        nodes_feat = np.load(re.sub('.edges.csv$', '.nfeat.npy', edgep))\n",
    "        nodes_label = pd.read_csv(\n",
    "            re.sub('.edges.csv$', '.idx.csv', edgep), encoding='utf-8')\n",
    "        n_nodes = self._get_n_nodes(nodes_label)\n",
    "        \n",
    "        nodes_label = nodes_label['label'].astype('category').cat.codes.to_list()\n",
    "        edge = pd.read_csv(edgep, encoding='utf-8')\n",
    "        \n",
    "        g = dgl.graph((edge['src'], edge['dst']), num_nodes=n_nodes)\n",
    "        g = dgl.to_bidirected(g)\n",
    "        g = dgl.remove_self_loop(g)\n",
    "        g = dgl.add_self_loop(g)\n",
    "        g.ndata['feat' ] = torch.from_numpy(nodes_feat )\n",
    "        g.ndata['label'] = torch.tensor    (nodes_label)\n",
    "        # g.ndata['train_mask'] = torch.ones (n_nodes, dtype=torch.bool)\n",
    "        # g.ndata['val_mask'  ] = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        # g.ndata['test_mask' ] = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def shuffle(self): shuffle(self.edges)\n",
    "        \n",
    "    def process(self): ...\n",
    "    \n",
    "train_data = DkkdGraphDataset(root='dataset/DKKD_graph')\n",
    "val_data = DkkdGraphDataset(root='dataset/DKKD_graph_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9888, 0.9505, 0.9949, 0.9913, 0.9944, 0.9973, 0.9901, 0.9851, 0.9859,\n",
      "        0.9840, 0.9851, 0.9543, 0.9950, 0.9968, 0.9975, 0.9988, 0.9945, 0.9973,\n",
      "        0.9916, 0.9913, 0.9809, 0.9885, 0.9901, 0.9975, 0.9942, 0.9974, 0.9735,\n",
      "        0.9631, 0.9944, 0.9816, 0.9802, 0.9525, 0.9943, 0.9944, 0.9944, 0.9944,\n",
      "        0.9972, 0.9836, 0.9972, 0.9801, 0.9854, 0.9876, 0.9934, 0.9727, 0.9777,\n",
      "        0.9861, 0.9941, 0.6093], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def cacu_alpha() -> torch.Tensor:\n",
    "    df = pd.read_csv('dataset/DKKD/classes_unbalance.csv', encoding='utf-8')\n",
    "    classes = df['class_name']\n",
    "    num = df['num']\n",
    "    stat_cls = []\n",
    "    for cls, n_node in zip(classes, num):\n",
    "        idx = class_2_idx(cls)\n",
    "        stat_cls.append((idx, n_node))\n",
    "        \n",
    "    stat_cls = np.array(sorted(stat_cls, key=lambda x:x[0]))\n",
    "    tong = 1.*stat_cls[:,1].sum()\n",
    "    freq = stat_cls[:,1]/tong\n",
    "    # alpha = 1/freq\n",
    "    alpha = 1.0 - freq\n",
    "    return torch.from_numpy(alpha)\n",
    "alpha = cacu_alpha()\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val(val_dataset:DGLDataset, model:nn.Module) -> float:\n",
    "    acc = 0.0\n",
    "    for g in val_dataset:\n",
    "        features = g.ndata['feat'].float()\n",
    "        labels = g.ndata['label']\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(1)\n",
    "        acc += (pred == labels).float().mean()\n",
    "    return acc/len(val_dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_with_filter_cls(val_dataset:DGLDataset, model:nn.Module, \n",
    "                        ignore_class:Optional[Tuple[int, ...]]=None) -> float:\n",
    "    if ignore_class is None: \n",
    "        return val(val_dataset, model)\n",
    "    \n",
    "    acc = 0.0\n",
    "    for g in val_dataset:\n",
    "        features = g.ndata['feat'].float()\n",
    "        labels = g.ndata['label']\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        filter_cls = [y for y, lbl in zip(pred == labels, labels) \n",
    "                      if lbl not in ignore_class]\n",
    "        acc += torch.tensor(filter_cls).float().mean()\n",
    "    return acc / len(val_dataset)\n",
    "    \n",
    "    \n",
    "def train_n_valid(train_dataset:DGLDataset, val_dataset:DGLDataset, \n",
    "                  model:nn.Module, epochs:int, lr=0.001, path_save:str=...):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "    # optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9,0.999), eps=1e-8, \n",
    "    #                       rectify = False, print_change_log=False)\n",
    "    criteron:FocalLoss = focal_loss(alpha= alpha, gamma=4.2)\n",
    "    best_acc = -1.0\n",
    "    # model.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        idxs = list(range(len(train_dataset)));shuffle(idxs)\n",
    "        acc = 0\n",
    "        for i in idxs:\n",
    "            g = train_dataset[i]\n",
    "\n",
    "            features = g.ndata['feat'].float()\n",
    "            labels = g.ndata['label']\n",
    "            \n",
    "            logits = model(g, features)\n",
    "            pred = logits.argmax(1)\n",
    "            \n",
    "            loss = criteron(logits, labels)\n",
    "            acc += (pred == labels).float().mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_acc = (1.*acc)/len(train_dataset)\n",
    "        # val_acc = val(val_dataset, model)\n",
    "        val_acc_filter32 = val_with_filter_cls(val_dataset, model, (32,))\n",
    "        if val_acc_filter32 >= best_acc:\n",
    "            best_acc = val_acc_filter32\n",
    "            best_model = deepcopy(model)\n",
    "            torch.save(model.state_dict(), path_save)\n",
    "        \n",
    "        if not epoch%32:\n",
    "            print('Epoch {:<3d}: loss: {:.4f}, best {:.4f}, train_acc: {:.4f}, val_filter: {:.4f}'.format(\n",
    "                            epoch, loss, best_acc, train_acc, val_acc_filter32))\n",
    "\n",
    "    torch.save(best_model.state_dict(), path_save)\n",
    "    print('Last epoch {:<3d}: loss: {:.4f}, best {:.4f}, train_acc: {:.4f}, val_filter: {:.4f}'.format(\n",
    "                    epoch, loss, best_acc, train_acc, val_acc_filter32))\n",
    "    \n",
    "    return best_model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes =  48\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(classes)  #48\n",
    "print('num classes = ', num_classes)\n",
    "model = GraphSAGE(772, 196, 64, num_classes) #\n",
    "# model.load_state_dict(torch.load('weights/graphSAGE_best40.pth',\n",
    "#                                  map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decoupling enabled in AdaBelief\n",
      "Epoch 32 : loss: 1.7213, best 0.3882, train_acc: 0.3444, val_filter: 0.3498\n",
      "Epoch 64 : loss: 1.4887, best 0.5081, train_acc: 0.5316, val_filter: 0.4086\n",
      "Epoch 96 : loss: 0.0353, best 0.5625, train_acc: 0.6053, val_filter: 0.4673\n",
      "Epoch 128: loss: 0.3313, best 0.5880, train_acc: 0.6876, val_filter: 0.4227\n",
      "Epoch 160: loss: 0.2928, best 0.5880, train_acc: 0.7388, val_filter: 0.4245\n",
      "Epoch 192: loss: 0.5892, best 0.5880, train_acc: 0.7336, val_filter: 0.3532\n",
      "Epoch 224: loss: 0.1088, best 0.5880, train_acc: 0.8079, val_filter: 0.5457\n",
      "Epoch 256: loss: 0.0792, best 0.5880, train_acc: 0.8209, val_filter: 0.3926\n",
      "Epoch 288: loss: 0.0811, best 0.5880, train_acc: 0.8530, val_filter: 0.5250\n",
      "Epoch 320: loss: 0.0751, best 0.5880, train_acc: 0.8696, val_filter: 0.5291\n",
      "Epoch 352: loss: 0.0020, best 0.5880, train_acc: 0.9205, val_filter: 0.5303\n",
      "Epoch 384: loss: 0.1404, best 0.5880, train_acc: 0.8882, val_filter: 0.5311\n",
      "Epoch 416: loss: 0.0016, best 0.5880, train_acc: 0.9366, val_filter: 0.4917\n",
      "Epoch 448: loss: 0.0817, best 0.5880, train_acc: 0.9431, val_filter: 0.5426\n",
      "Epoch 480: loss: 0.0643, best 0.5880, train_acc: 0.9371, val_filter: 0.5367\n",
      "Epoch 512: loss: 0.0236, best 0.5880, train_acc: 0.9664, val_filter: 0.5449\n",
      "Epoch 544: loss: 0.0052, best 0.5880, train_acc: 0.9697, val_filter: 0.5179\n",
      "Epoch 576: loss: 0.0000, best 0.5880, train_acc: 0.9784, val_filter: 0.5108\n",
      "Epoch 608: loss: 0.0128, best 0.5880, train_acc: 0.9864, val_filter: 0.5373\n",
      "Epoch 640: loss: 0.0037, best 0.5880, train_acc: 0.9885, val_filter: 0.5392\n",
      "Epoch 672: loss: 0.0014, best 0.5880, train_acc: 0.9902, val_filter: 0.5241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000009?line=0'>1</a>\u001b[0m best_model, best_acc \u001b[39m=\u001b[39m train_n_valid(train_data, val_data, model, \u001b[39m2500\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, path_save\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweights/graphSAGE_best40.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000009?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39msave(best_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mweights/graphSAGE_best40.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(best_acc)\n",
      "\u001b[1;32m/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain_n_valid\u001b[0;34m(train_dataset, val_dataset, model, epochs, lr, path_save)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000007?line=44'>45</a>\u001b[0m features \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000007?line=45'>46</a>\u001b[0m labels \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000007?line=47'>48</a>\u001b[0m logits \u001b[39m=\u001b[39m model(g, features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000007?line=48'>49</a>\u001b[0m pred \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000007?line=50'>51</a>\u001b[0m loss \u001b[39m=\u001b[39m criteron(logits, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb Cell 3'\u001b[0m in \u001b[0;36mGraphSAGE.forward\u001b[0;34m(self, g, in_feat)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, in_feat):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=48'>49</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(g, in_feat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=49'>50</a>\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=50'>51</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(g, h)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb Cell 3'\u001b[0m in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=27'>28</a>\u001b[0m g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m h\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=28'>29</a>\u001b[0m \u001b[39m# update_all is a message passing API.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=29'>30</a>\u001b[0m g\u001b[39m.\u001b[39;49mupdate_all(message_func \u001b[39m=\u001b[39;49m fn\u001b[39m.\u001b[39;49mcopy_u(\u001b[39m'\u001b[39;49m\u001b[39mh\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m'\u001b[39;49m), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=30'>31</a>\u001b[0m              reduce_func \u001b[39m=\u001b[39;49m fn\u001b[39m.\u001b[39;49mmean(\u001b[39m'\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mh_N\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=31'>32</a>\u001b[0m             \u001b[39m#  reduce_func = fn.max('m', 'h_N'))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=32'>33</a>\u001b[0m             \u001b[39m#  reduce_func = fn.sum('m', 'h_N'))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=33'>34</a>\u001b[0m             \u001b[39m#  reduce_func = fn.sum('m', 'h_N'))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agent/Documents/graph/GNN/graphSAGE_with_dkkd.ipynb#ch0000002?line=34'>35</a>\u001b[0m h_N \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mh_N\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/heterograph.py:4895\u001b[0m, in \u001b[0;36mDGLHeteroGraph.update_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func, etype)\u001b[0m\n\u001b[1;32m   4893\u001b[0m _, dtid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mmetagraph\u001b[39m.\u001b[39mfind_edge(etid)\n\u001b[1;32m   4894\u001b[0m g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m etype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m[etype]\n\u001b[0;32m-> 4895\u001b[0m ndata \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mmessage_passing(g, message_func, reduce_func, apply_node_func)\n\u001b[1;32m   4896\u001b[0m \u001b[39mif\u001b[39;00m core\u001b[39m.\u001b[39mis_builtin(reduce_func) \u001b[39mand\u001b[39;00m reduce_func\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m ndata:\n\u001b[1;32m   4897\u001b[0m     \u001b[39m# Replace infinity with zero for isolated nodes\u001b[39;00m\n\u001b[1;32m   4898\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ndata\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/core.py:357\u001b[0m, in \u001b[0;36mmessage_passing\u001b[0;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"Invoke message passing computation on the whole graph.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m    Results from the message passing computation.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (is_builtin(mfunc) \u001b[39mand\u001b[39;00m is_builtin(rfunc) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         \u001b[39mgetattr\u001b[39m(ops, \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(mfunc\u001b[39m.\u001b[39mname, rfunc\u001b[39m.\u001b[39mname), \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    356\u001b[0m     \u001b[39m# invoke fused message passing\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     ndata \u001b[39m=\u001b[39m invoke_gspmm(g, mfunc, rfunc)\n\u001b[1;32m    358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39m# invoke message passing in two separate steps\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# message phase\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m is_builtin(mfunc):\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/core.py:332\u001b[0m, in \u001b[0;36minvoke_gspmm\u001b[0;34m(graph, mfunc, rfunc, srcdata, dstdata, edata)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[39melse\u001b[39;00m: \u001b[39m# \"copy_e\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m             x \u001b[39m=\u001b[39m data_dict_to_list(graph, x, mfunc, \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m     z \u001b[39m=\u001b[39m op(graph, x)\n\u001b[1;32m    333\u001b[0m \u001b[39mreturn\u001b[39;00m {rfunc\u001b[39m.\u001b[39mout_field : z}\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/ops/spmm.py:189\u001b[0m, in \u001b[0;36m_gen_copy_reduce_func.<locals>.func\u001b[0;34m(g, x)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(g, x):\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m binary_op \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcopy_u\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mreturn\u001b[39;00m gspmm(g, \u001b[39m'\u001b[39;49m\u001b[39mcopy_lhs\u001b[39;49m\u001b[39m'\u001b[39;49m, reduce_op, x, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    190\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[39mreturn\u001b[39;00m gspmm(g, \u001b[39m'\u001b[39m\u001b[39mcopy_rhs\u001b[39m\u001b[39m'\u001b[39m, reduce_op, \u001b[39mNone\u001b[39;00m, x)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/ops/spmm.py:75\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(g, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m         lhs_data, rhs_data \u001b[39m=\u001b[39m reshape_lhs_rhs(lhs_data, rhs_data)\n\u001b[1;32m     74\u001b[0m     \u001b[39m# With max and min reducers infinity will be returned for zero degree nodes\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     ret \u001b[39m=\u001b[39m gspmm_internal(g\u001b[39m.\u001b[39;49m_graph, op,\n\u001b[1;32m     76\u001b[0m                          \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m reduce_op \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39melse\u001b[39;49;00m reduce_op,\n\u001b[1;32m     77\u001b[0m                          lhs_data, rhs_data)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[39m# lhs_data or rhs_data is None only in unary functions like ``copy-u`` or ``copy_e``\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     lhs_data \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m g\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mnumber_of_ntypes() \u001b[39mif\u001b[39;00m lhs_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lhs_data\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/backend/pytorch/sparse.py:757\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m    755\u001b[0m     op \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmul\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    756\u001b[0m     rhs_data \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m rhs_data\n\u001b[0;32m--> 757\u001b[0m \u001b[39mreturn\u001b[39;00m GSpMM\u001b[39m.\u001b[39;49mapply(gidx, op, reduce_op, lhs_data, rhs_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:94\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m fwd(\u001b[39m*\u001b[39m_cast(args, cast_inputs), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_cast(kwargs, cast_inputs))\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m fwd(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/backend/pytorch/sparse.py:126\u001b[0m, in \u001b[0;36mGSpMM.forward\u001b[0;34m(ctx, gidx, op, reduce_op, X, Y)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39m@custom_fwd\u001b[39m(cast_inputs\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, gidx, op, reduce_op, X, Y):\n\u001b[0;32m--> 126\u001b[0m     out, (argX, argY) \u001b[39m=\u001b[39m _gspmm(gidx, op, reduce_op, X, Y)\n\u001b[1;32m    127\u001b[0m     reduce_last \u001b[39m=\u001b[39m _need_reduce_last_dim(X, Y)\n\u001b[1;32m    128\u001b[0m     X_shape \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/sparse.py:216\u001b[0m, in \u001b[0;36m_gspmm\u001b[0;34m(gidx, op, reduce_op, u, e)\u001b[0m\n\u001b[1;32m    213\u001b[0m _, dsttype \u001b[39m=\u001b[39m gidx\u001b[39m.\u001b[39mmetagraph\u001b[39m.\u001b[39mfind_edge(\u001b[39m0\u001b[39m)\n\u001b[1;32m    214\u001b[0m v_shp \u001b[39m=\u001b[39m (gidx\u001b[39m.\u001b[39mnumber_of_nodes(dsttype), ) \u001b[39m+\u001b[39m\\\n\u001b[1;32m    215\u001b[0m     infer_broadcast_shape(op, u_shp[\u001b[39m1\u001b[39m:], e_shp[\u001b[39m1\u001b[39m:])\n\u001b[0;32m--> 216\u001b[0m v \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mzeros(v_shp, dtype, ctx)\n\u001b[1;32m    217\u001b[0m use_cmp \u001b[39m=\u001b[39m reduce_op \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    218\u001b[0m arg_u, arg_e \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:226\u001b[0m, in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, ctx)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzeros\u001b[39m(shape, dtype, ctx):\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39;49mzeros(shape, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mctx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model, best_acc = train_n_valid(train_data, val_data, model, 2500, lr=0.001, path_save='weights/graphSAGE_best40.pth')\n",
    "torch.save(best_model.state_dict(), 'weights/graphSAGE_best40.pth')\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d620334016f14c166285ee8528a11ee9bc398b5cacff66726993ff9e9a83d46b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
