{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import DGLDataset, CoraGraphDataset\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "#NOTE cora have only one graph\n",
    "g = dataset[0]      # def __getitem__(self, idx) assert idx == 0, \"This dataset has only one graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 7\n",
      "Node features\n",
      "\u001b[31m feat \u001b[39m\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0526, 0.0000]])\n",
      "\u001b[31m label \u001b[39m\n",
      "tensor([4, 4, 4,  ..., 4, 3, 3])\n",
      "\u001b[31m test_mask \u001b[39m\n",
      "tensor([ True,  True, False,  ..., False, False, False])\n",
      "\u001b[31m val_mask \u001b[39m\n",
      "tensor([False, False,  True,  ..., False, False, False])\n",
      "\u001b[31m train_mask \u001b[39m\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "shape[0] is number of node\n",
      "shape[1] is length of vector node feature\n",
      "\n",
      "shape of node feature matrix X:  torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "print('Number of categories:', dataset.num_classes)\n",
    "\n",
    "\n",
    "def show_graph_data():\n",
    "    #NOTE show node/edge feature\n",
    "    print('Node features')\n",
    "    for k, v in g.ndata.items():\n",
    "        print(Fore.RED, k, Fore.RESET)\n",
    "        print(v)\n",
    "    \n",
    "\n",
    "    print('shape[0] is number of node') \n",
    "    print('shape[1] is length of vector node feature')\n",
    "    print('\\nshape of node feature matrix X: ', g.ndata['feat'].shape)\n",
    "    # print('Edge features')\n",
    "    # for k, v in g.edata.items():\n",
    "    #     print(k, v)\n",
    "\n",
    "show_graph_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defind GNN architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g:DGLDataset, model:nn.Module, epochs:int):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not epoch % 5:\n",
    "            print('In epoch {:3d}, loss: {:.4f}, val acc: {:.4f} (best {:.4f}), test acc: {:.4f} (best {:.4f})'.format(\n",
    "                epoch, loss, val_acc, best_val_acc, test_acc, best_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agent/anaconda3/envs/graph/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch   5, loss: 1.9084, val acc: 0.3700 (best 0.3700), test acc: 0.4050 (best 0.4050)\n",
      "In epoch  10, loss: 1.8400, val acc: 0.4700 (best 0.4700), test acc: 0.5000 (best 0.5000)\n",
      "In epoch  15, loss: 1.7463, val acc: 0.5040 (best 0.5040), test acc: 0.5460 (best 0.5460)\n",
      "In epoch  20, loss: 1.6289, val acc: 0.5880 (best 0.5880), test acc: 0.6200 (best 0.6200)\n",
      "In epoch  25, loss: 1.4896, val acc: 0.6440 (best 0.6440), test acc: 0.6840 (best 0.6840)\n",
      "In epoch  30, loss: 1.3316, val acc: 0.7040 (best 0.7040), test acc: 0.7150 (best 0.7100)\n",
      "In epoch  35, loss: 1.1621, val acc: 0.7400 (best 0.7400), test acc: 0.7330 (best 0.7330)\n",
      "In epoch  40, loss: 0.9905, val acc: 0.7500 (best 0.7520), test acc: 0.7430 (best 0.7400)\n",
      "In epoch  45, loss: 0.8267, val acc: 0.7660 (best 0.7660), test acc: 0.7580 (best 0.7580)\n",
      "In epoch  50, loss: 0.6787, val acc: 0.7780 (best 0.7780), test acc: 0.7680 (best 0.7680)\n",
      "In epoch  55, loss: 0.5510, val acc: 0.7900 (best 0.7900), test acc: 0.7660 (best 0.7660)\n",
      "In epoch  60, loss: 0.4448, val acc: 0.7920 (best 0.7920), test acc: 0.7750 (best 0.7710)\n",
      "In epoch  65, loss: 0.3589, val acc: 0.7880 (best 0.7940), test acc: 0.7830 (best 0.7750)\n",
      "In epoch  70, loss: 0.2907, val acc: 0.7880 (best 0.7940), test acc: 0.7820 (best 0.7750)\n",
      "In epoch  75, loss: 0.2371, val acc: 0.7860 (best 0.7940), test acc: 0.7820 (best 0.7750)\n",
      "In epoch  80, loss: 0.1951, val acc: 0.7840 (best 0.7940), test acc: 0.7820 (best 0.7750)\n",
      "In epoch  85, loss: 0.1623, val acc: 0.7820 (best 0.7940), test acc: 0.7820 (best 0.7750)\n",
      "In epoch  90, loss: 0.1364, val acc: 0.7840 (best 0.7940), test acc: 0.7810 (best 0.7750)\n",
      "In epoch  95, loss: 0.1159, val acc: 0.7840 (best 0.7940), test acc: 0.7800 (best 0.7750)\n",
      "In epoch 100, loss: 0.0995, val acc: 0.7860 (best 0.7940), test acc: 0.7750 (best 0.7750)\n",
      "In epoch 105, loss: 0.0862, val acc: 0.7840 (best 0.7940), test acc: 0.7730 (best 0.7750)\n",
      "In epoch 110, loss: 0.0754, val acc: 0.7840 (best 0.7940), test acc: 0.7700 (best 0.7750)\n",
      "In epoch 115, loss: 0.0666, val acc: 0.7840 (best 0.7940), test acc: 0.7690 (best 0.7750)\n",
      "In epoch 120, loss: 0.0592, val acc: 0.7840 (best 0.7940), test acc: 0.7690 (best 0.7750)\n",
      "In epoch 125, loss: 0.0530, val acc: 0.7840 (best 0.7940), test acc: 0.7680 (best 0.7750)\n",
      "In epoch 130, loss: 0.0478, val acc: 0.7860 (best 0.7940), test acc: 0.7700 (best 0.7750)\n",
      "In epoch 135, loss: 0.0434, val acc: 0.7840 (best 0.7940), test acc: 0.7670 (best 0.7750)\n",
      "In epoch 140, loss: 0.0396, val acc: 0.7800 (best 0.7940), test acc: 0.7680 (best 0.7750)\n",
      "In epoch 145, loss: 0.0362, val acc: 0.7800 (best 0.7940), test acc: 0.7650 (best 0.7750)\n",
      "In epoch 150, loss: 0.0333, val acc: 0.7780 (best 0.7940), test acc: 0.7650 (best 0.7750)\n",
      "In epoch 155, loss: 0.0308, val acc: 0.7780 (best 0.7940), test acc: 0.7650 (best 0.7750)\n",
      "In epoch 160, loss: 0.0286, val acc: 0.7780 (best 0.7940), test acc: 0.7640 (best 0.7750)\n",
      "In epoch 165, loss: 0.0266, val acc: 0.7760 (best 0.7940), test acc: 0.7640 (best 0.7750)\n",
      "In epoch 170, loss: 0.0248, val acc: 0.7760 (best 0.7940), test acc: 0.7630 (best 0.7750)\n",
      "In epoch 175, loss: 0.0232, val acc: 0.7760 (best 0.7940), test acc: 0.7630 (best 0.7750)\n",
      "In epoch 180, loss: 0.0217, val acc: 0.7760 (best 0.7940), test acc: 0.7630 (best 0.7750)\n",
      "In epoch 185, loss: 0.0204, val acc: 0.7760 (best 0.7940), test acc: 0.7630 (best 0.7750)\n",
      "In epoch 190, loss: 0.0193, val acc: 0.7780 (best 0.7940), test acc: 0.7610 (best 0.7750)\n",
      "In epoch 195, loss: 0.0182, val acc: 0.7760 (best 0.7940), test acc: 0.7590 (best 0.7750)\n",
      "In epoch 200, loss: 0.0172, val acc: 0.7740 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 205, loss: 0.0163, val acc: 0.7740 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 210, loss: 0.0155, val acc: 0.7740 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 215, loss: 0.0147, val acc: 0.7740 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 220, loss: 0.0140, val acc: 0.7720 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 225, loss: 0.0133, val acc: 0.7720 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 230, loss: 0.0127, val acc: 0.7720 (best 0.7940), test acc: 0.7580 (best 0.7750)\n",
      "In epoch 235, loss: 0.0122, val acc: 0.7720 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 240, loss: 0.0116, val acc: 0.7720 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 245, loss: 0.0111, val acc: 0.7720 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 250, loss: 0.0107, val acc: 0.7720 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 255, loss: 0.0102, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 260, loss: 0.0098, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 265, loss: 0.0095, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 270, loss: 0.0091, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 275, loss: 0.0088, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 280, loss: 0.0085, val acc: 0.7720 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 285, loss: 0.0082, val acc: 0.7740 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 290, loss: 0.0079, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 295, loss: 0.0076, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 300, loss: 0.0073, val acc: 0.7760 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 305, loss: 0.0071, val acc: 0.7760 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 310, loss: 0.0069, val acc: 0.7760 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 315, loss: 0.0067, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 320, loss: 0.0065, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 325, loss: 0.0063, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 330, loss: 0.0061, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 335, loss: 0.0059, val acc: 0.7780 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 340, loss: 0.0057, val acc: 0.7780 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 345, loss: 0.0056, val acc: 0.7780 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 350, loss: 0.0054, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 355, loss: 0.0052, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 360, loss: 0.0051, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 365, loss: 0.0050, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 370, loss: 0.0048, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 375, loss: 0.0047, val acc: 0.7760 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 380, loss: 0.0046, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 385, loss: 0.0045, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 390, loss: 0.0044, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 395, loss: 0.0043, val acc: 0.7740 (best 0.7940), test acc: 0.7570 (best 0.7750)\n",
      "In epoch 400, loss: 0.0041, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 405, loss: 0.0040, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 410, loss: 0.0040, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 415, loss: 0.0039, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 420, loss: 0.0038, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 425, loss: 0.0037, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 430, loss: 0.0036, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 435, loss: 0.0035, val acc: 0.7720 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 440, loss: 0.0034, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 445, loss: 0.0034, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 450, loss: 0.0033, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 455, loss: 0.0032, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 460, loss: 0.0032, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 465, loss: 0.0031, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 470, loss: 0.0030, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 475, loss: 0.0030, val acc: 0.7720 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 480, loss: 0.0029, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 485, loss: 0.0028, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 490, loss: 0.0028, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 495, loss: 0.0027, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 500, loss: 0.0027, val acc: 0.7740 (best 0.7940), test acc: 0.7560 (best 0.7750)\n",
      "In epoch 505, loss: 0.0026, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 510, loss: 0.0026, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 515, loss: 0.0025, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 520, loss: 0.0025, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 525, loss: 0.0024, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 530, loss: 0.0024, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 535, loss: 0.0024, val acc: 0.7740 (best 0.7940), test acc: 0.7550 (best 0.7750)\n",
      "In epoch 540, loss: 0.0023, val acc: 0.7700 (best 0.7940), test acc: 0.7540 (best 0.7750)\n",
      "In epoch 545, loss: 0.0023, val acc: 0.7700 (best 0.7940), test acc: 0.7540 (best 0.7750)\n",
      "In epoch 550, loss: 0.0022, val acc: 0.7700 (best 0.7940), test acc: 0.7540 (best 0.7750)\n",
      "In epoch 555, loss: 0.0022, val acc: 0.7700 (best 0.7940), test acc: 0.7540 (best 0.7750)\n",
      "In epoch 560, loss: 0.0022, val acc: 0.7700 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 565, loss: 0.0021, val acc: 0.7700 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 570, loss: 0.0021, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 575, loss: 0.0020, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 580, loss: 0.0020, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 585, loss: 0.0020, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 590, loss: 0.0019, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 595, loss: 0.0019, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 600, loss: 0.0019, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 605, loss: 0.0019, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 610, loss: 0.0018, val acc: 0.7680 (best 0.7940), test acc: 0.7530 (best 0.7750)\n",
      "In epoch 615, loss: 0.0018, val acc: 0.7680 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 620, loss: 0.0018, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 625, loss: 0.0017, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 630, loss: 0.0017, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 635, loss: 0.0017, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 640, loss: 0.0017, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 645, loss: 0.0016, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 650, loss: 0.0016, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 655, loss: 0.0016, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 660, loss: 0.0016, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 665, loss: 0.0015, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 670, loss: 0.0015, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 675, loss: 0.0015, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 680, loss: 0.0015, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 685, loss: 0.0015, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 690, loss: 0.0014, val acc: 0.7680 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 695, loss: 0.0014, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 700, loss: 0.0014, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 705, loss: 0.0014, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 710, loss: 0.0014, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 715, loss: 0.0013, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 720, loss: 0.0013, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 725, loss: 0.0013, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 730, loss: 0.0013, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 735, loss: 0.0013, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 740, loss: 0.0013, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 745, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 750, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 755, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 760, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 765, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 770, loss: 0.0012, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 775, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 780, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 785, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 790, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 795, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 800, loss: 0.0011, val acc: 0.7700 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 805, loss: 0.0011, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 810, loss: 0.0011, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 815, loss: 0.0010, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 820, loss: 0.0010, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 825, loss: 0.0010, val acc: 0.7680 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 830, loss: 0.0010, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 835, loss: 0.0010, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 840, loss: 0.0010, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 845, loss: 0.0010, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 850, loss: 0.0010, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 855, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 860, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 865, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 870, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 875, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 880, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7500 (best 0.7750)\n",
      "In epoch 885, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 890, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 895, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 900, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 905, loss: 0.0009, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 910, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 915, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 920, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 925, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 930, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 935, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 940, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 945, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 950, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 955, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 960, loss: 0.0008, val acc: 0.7660 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 965, loss: 0.0007, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 970, loss: 0.0007, val acc: 0.7660 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 975, loss: 0.0007, val acc: 0.7660 (best 0.7940), test acc: 0.7510 (best 0.7750)\n",
      "In epoch 980, loss: 0.0007, val acc: 0.7660 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 985, loss: 0.0007, val acc: 0.7660 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 990, loss: 0.0007, val acc: 0.7640 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 995, loss: 0.0007, val acc: 0.7640 (best 0.7940), test acc: 0.7520 (best 0.7750)\n",
      "In epoch 1000, loss: 0.0007, val acc: 0.7640 (best 0.7940), test acc: 0.7520 (best 0.7750)\n"
     ]
    }
   ],
   "source": [
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g.to('cuda')\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
    "train(g, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d620334016f14c166285ee8528a11ee9bc398b5cacff66726993ff9e9a83d46b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
